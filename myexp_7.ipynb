{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d0b7dc3-396f-4044-9687-ed760cdf992d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in d:\\datascience\\myenv\\lib\\site-packages (3.9.1)\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [15 lines of output]\n",
      "      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "      rather than 'sklearn' for pip commands.\n",
      "      \n",
      "      Here is how to fix this error in the main use cases:\n",
      "      - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "      - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "      - if the 'sklearn' package is used by one of your dependencies,\n",
      "        it would be great if you take some time to track which package uses\n",
      "        'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "      - as a last resort, set the environment variable\n",
      "        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "      \n",
      "      More information is available at\n",
      "      https://github.com/scikit-learn/sklearn-pypi-package\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nltk sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aeb287f-f842-490c-9305-ef8ac083c254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Rahul\n",
      "[nltk_data]     Wanjare\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Rahul\n",
      "[nltk_data]     Wanjare\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Rahul\n",
      "[nltk_data]     Wanjare\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Rahul Wanjare\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Download required NLTK data (run only once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8d5a2b9-c143-48ed-acee-cc7175da7ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Cats are running around the garden happily.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b368b2a-ac90-4187-bc2b-616f221b5575",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.lower()\n",
    "text = \"\".join([char for char in text if char not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30ecb078-5aa8-42d0-bee9-b8214f646e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered words: ['cats', 'running', 'around', 'garden', 'happily']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(text)\n",
    "stop_words = stopwords.words('english')\n",
    "filtered = [word for word in tokens if word not in stop_words]\n",
    "print(\"Filtered words:\", filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33d3bf8d-3d5a-4648-8ea0-de8196ac4198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed words: ['cat', 'run', 'around', 'garden', 'happili']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(word) for word in filtered]\n",
    "print(\"Stemmed words:\", stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d92aeeb0-950c-4542-9de3-14bee6a11811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized words: ['cat', 'running', 'around', 'garden', 'happily']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in filtered]\n",
    "print(\"Lemmatized words:\", lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b0f0a79-a707-4cb0-bd7d-0ad423848057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Words: ['around' 'cat' 'garden' 'happily' 'running']\n",
      "TF-IDF Scores: [[0.4472136 0.4472136 0.4472136 0.4472136 0.4472136]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Join words back into a sentence\n",
    "doc = \" \".join(lemmatized)\n",
    "\n",
    "# TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform([doc])\n",
    "\n",
    "print(\"TF-IDF Words:\", tfidf.get_feature_names_out())\n",
    "print(\"TF-IDF Scores:\", X.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f73047-08e3-4376-9251-c54bf1fc1cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
