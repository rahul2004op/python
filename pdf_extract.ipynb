{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "962665aa-e682-403d-8b10-d808fc0d8e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Downloading pymupdf-1.25.5-cp39-abi3-win_amd64.whl (16.6 MB)\n",
      "     ---------------------------------------- 0.0/16.6 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/16.6 MB 9.9 MB/s eta 0:00:02\n",
      "     -- ------------------------------------- 0.9/16.6 MB 11.4 MB/s eta 0:00:02\n",
      "     --- ------------------------------------ 1.5/16.6 MB 10.2 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 2.0/16.6 MB 11.4 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 2.5/16.6 MB 11.5 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 3.1/16.6 MB 11.5 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 3.6/16.6 MB 11.5 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 4.1/16.6 MB 11.3 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 4.7/16.6 MB 11.5 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 5.3/16.6 MB 11.6 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 5.7/16.6 MB 11.4 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 6.3/16.6 MB 11.5 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 6.8/16.6 MB 11.4 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 7.3/16.6 MB 11.4 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 7.9/16.6 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 8.2/16.6 MB 11.4 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 8.3/16.6 MB 11.3 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 8.6/16.6 MB 10.4 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 9.2/16.6 MB 10.5 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 9.7/16.6 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------ -------------- 10.3/16.6 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------- ------------- 10.9/16.6 MB 10.7 MB/s eta 0:00:01\n",
      "     -------------------------- ------------ 11.3/16.6 MB 10.9 MB/s eta 0:00:01\n",
      "     --------------------------- ----------- 11.9/16.6 MB 10.9 MB/s eta 0:00:01\n",
      "     ----------------------------- --------- 12.3/16.6 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 13.0/16.6 MB 10.9 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 13.5/16.6 MB 10.7 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 14.0/16.6 MB 10.7 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 14.5/16.6 MB 10.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 15.0/16.6 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 15.5/16.6 MB 10.6 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 15.5/16.6 MB 10.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 15.8/16.6 MB 10.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  16.5/16.6 MB 10.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  16.6/16.6 MB 10.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 16.6/16.6 MB 9.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: nltk in d:\\datascience\\myenv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in d:\\datascience\\myenv\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: joblib in d:\\datascience\\myenv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: tqdm in d:\\datascience\\myenv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: click in d:\\datascience\\myenv\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\datascience\\myenv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\datascience\\myenv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in d:\\datascience\\myenv\\lib\\site-packages (from scikit-learn) (2.2.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\datascience\\myenv\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: colorama in d:\\datascience\\myenv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: PyMuPDF\n",
      "Successfully installed PyMuPDF-1.25.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install PyMuPDF nltk scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00e503f1-1df5-4a33-a81b-620df400d14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Rahul\n",
      "[nltk_data]     Wanjare\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Rahul\n",
      "[nltk_data]     Wanjare\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Rahul Wanjare\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Rahul\n",
      "[nltk_data]     Wanjare\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original PDF Text (first 300 characters):\n",
      "\n",
      "Welcome to Smallpdf\n",
      "Digital Documents—All In One Place\n",
      "Access Files Anytime, Anywhere \n",
      "Enhance Documents in One Click \n",
      "Collaborate With Others \n",
      "With the new Smallpdf experience, you can \n",
      "freely upload, organize, and share digital \n",
      "documents. When you enable the ‘Storage’ \n",
      "option, we’ll also store al\n",
      "\n",
      "Tokens:\n",
      " ['Welcome', 'to', 'Smallpdf', 'Digital', 'Documents—All', 'In', 'One', 'Place', 'Access', 'Files', 'Anytime', ',', 'Anywhere', 'Enhance', 'Documents', 'in', 'One', 'Click', 'Collaborate', 'With']\n",
      "\n",
      "POS Tags:\n",
      " [('Welcome', 'VB'), ('to', 'TO'), ('Smallpdf', 'NNP'), ('Digital', 'NNP'), ('Documents—All', 'NNP'), ('In', 'IN'), ('One', 'CD'), ('Place', 'NNP'), ('Access', 'NNP'), ('Files', 'NNP')]\n",
      "\n",
      "Filtered Tokens (no stopwords):\n",
      " ['Welcome', 'Smallpdf', 'Digital', 'One', 'Place', 'Access', 'Files', 'Anytime', 'Anywhere', 'Enhance', 'Documents', 'One', 'Click', 'Collaborate', 'Others', 'new', 'Smallpdf', 'experience', 'freely', 'upload']\n",
      "\n",
      "Stemmed Tokens:\n",
      " ['welcom', 'smallpdf', 'digit', 'one', 'place', 'access', 'file', 'anytim', 'anywher', 'enhanc', 'document', 'one', 'click', 'collabor', 'other', 'new', 'smallpdf', 'experi', 'freeli', 'upload']\n",
      "\n",
      "Lemmatized Tokens:\n",
      " ['Welcome', 'Smallpdf', 'Digital', 'One', 'Place', 'Access', 'Files', 'Anytime', 'Anywhere', 'Enhance', 'Documents', 'One', 'Click', 'Collaborate', 'Others', 'new', 'Smallpdf', 'experience', 'freely', 'upload']\n",
      "\n",
      "TF-IDF Feature Names:\n",
      " ['access' 'administrative' 'all' 'also' 'an' 'and' 'anytime' 'anywhere'\n",
      " 'app' 'array' 'can' 'click' 'collaborate' 'compress' 'computer' 'convert'\n",
      " 'digital' 'document' 'documents' 'enable']\n",
      "\n",
      "TF-IDF Matrix:\n",
      " [[0.11215443 0.05607722 0.11215443 0.11215443 0.05607722 0.05607722\n",
      "  0.05607722 0.05607722 0.11215443 0.05607722 0.16823165 0.11215443\n",
      "  0.05607722 0.05607722 0.05607722 0.05607722 0.11215443 0.05607722\n",
      "  0.16823165 0.11215443 0.05607722 0.05607722 0.05607722 0.05607722\n",
      "  0.05607722 0.28038608 0.05607722 0.05607722 0.05607722 0.11215443\n",
      "  0.05607722 0.11215443 0.05607722 0.05607722 0.05607722 0.16823165\n",
      "  0.05607722 0.05607722 0.05607722 0.05607722 0.05607722 0.05607722\n",
      "  0.05607722 0.11215443 0.11215443 0.05607722 0.05607722 0.05607722\n",
      "  0.16823165 0.05607722 0.05607722 0.05607722 0.05607722 0.05607722\n",
      "  0.05607722 0.05607722 0.05607722 0.05607722 0.05607722 0.05607722\n",
      "  0.05607722 0.05607722 0.05607722 0.05607722 0.33646329 0.05607722\n",
      "  0.05607722 0.05607722 0.05607722 0.05607722 0.05607722 0.05607722\n",
      "  0.05607722 0.28038608 0.28038608 0.05607722 0.16823165 0.05607722\n",
      "  0.11215443 0.22430886 0.33646329 0.11215443]]\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Step 1: Read PDF file and extract text\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# Change to your PDF file path\n",
    "pdf_text = extract_text_from_pdf(\"Sampledata_7.pdf\")  # <-- Replace with your file\n",
    "\n",
    "print(\"\\nOriginal PDF Text (first 300 characters):\\n\")\n",
    "print(pdf_text[:300])  # Preview only\n",
    "\n",
    "# Step 2: Text Preprocessing\n",
    "tokens = word_tokenize(pdf_text)\n",
    "print(\"\\nTokens:\\n\", tokens[:20])  # Preview tokens\n",
    "\n",
    "# POS Tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(\"\\nPOS Tags:\\n\", pos_tags[:10])  # Preview tags\n",
    "\n",
    "# Stopwords removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words and word.isalpha()]\n",
    "print(\"\\nFiltered Tokens (no stopwords):\\n\", filtered_tokens[:20])\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(\"\\nStemmed Tokens:\\n\", stemmed_tokens[:20])\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(\"\\nLemmatized Tokens:\\n\", lemmatized_tokens[:20])\n",
    "\n",
    "# Step 3: TF-IDF Representation\n",
    "documents = [pdf_text]  # Single-document example\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"\\nTF-IDF Feature Names:\\n\", vectorizer.get_feature_names_out()[:20])  # First 20 features\n",
    "print(\"\\nTF-IDF Matrix:\\n\", X.toarray())  # Show full TF-IDF values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827fc7c9-a722-46b4-a0a6-c5518b449d41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
